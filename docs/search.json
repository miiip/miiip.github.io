[
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "How to implement a quantum key distribution protocol on IBM's quantum computer (a practical example of quantum programming)\n    DefCamp 2019 presentation\n  \n\n  \n    Augment cybersecurity through A.I.\n    DefCamp 2022 presentation\n  \n\n  \n    The best of both worlds: privacy and security\n    DefCamp 2023 presentation\n  \n\n  \n    Singularize Your Neural Networks: Ensuring Privacy-Preserving Inference\n    DefCamp 2024 presentation\n  \n\n  \n    Saving the (post-quantum) world with neural networks at DefCamp Cluj-Napoca 2024\n    DefCamp 2024 Cluj Edition presentation\n  \n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Ple»ôa, M.I., Gheorghe, M. and Ipate, F. (2025) Neural Key Agreement Protocol with Extended Security. Applied Sciences, 15(23), 12746.\n  \n\n  \n    Ple»ôa, M.I., Poenaru, R., Irimia, S., David, S. and Farcasanu, A. (2025) Privacy-Preserving Inference for Public Neural Networks. In: International Conference on Artificial Intelligence and Soft Computing, pp. 120‚Äì131.\n  \n\n  \n    David, S., Ple»ôa, M.I., Irimia, S. and Poenaru, R. (2025) Privacy-Preserving Service for Secure Storage of Passwords Based on Singularization. In: 25th International Conference on Control Systems and Computer Science (CSCS 2025).\n  \n\n  \n    Macario-Rat, G. and Ple»ôa, M.I. (2024) Singularization: A New Approach to Designing Block Ciphers for Resource-Constrained Devices. In: International Conference on Attacks and Defenses for Internet-of-Things, pp. 155‚Äì167.\n  \n\n  \n    Zhang, G., Verlan, S., Wu, T., Cabarle, F.G.C., Xue, J., Orellana-Mart√≠n, D., Dong, J. et al. (2024) Spiking neural P systems: theory, applications and implementations. Springer Nature.\n  \n\n  \n    Ple»ôa, M.I., Gheorghe, M., Ipate, F. and Zhang, G. (2024) Applications of spiking neural P systems in cybersecurity. Journal of Membrane Computing, 6(4), pp. 310‚Äì317.\n  \n\n  \n    Ple»ôa, M.I., Gheorghe, M., Ipate, F. and Zhang, G. (2024) A federated learning protocol for spiking neural membrane systems. International Journal of Neural Systems, 34(12), 2450062.\n  \n\n  \n    Ple»ôa, M.I. and Olimid, R.F. (2024) Privacy-Preserving Multi-party Search via Homomorphic Encryption with Constant Multiplicative Depth. In: International Conference on Information Technology and Communications Security (SecITC 2024).\n  \n\n  \n    Ple»ôa, M.I., Gheorghe, M. and Ipate, F. (2024) Private Inference on Layered Spiking Neural P Systems. In: International Work-Conference on the Interplay Between Natural and Artificial Computation (IWINAC 2024).\n  \n\n  \n    Zentai, D., Ple»ôa, M.I. and Frot, R. (2023) A multiparty commutative hashing protocol based on the discrete logarithm problem. arXiv:2311.17498.\n  \n\n  \n    Ple»ôa, M.I., Gheorghe, M. and Ipate, F. (2023) Privacy-preserving Linear Computations in Spiking Neural P Systems. arXiv:2309.13803.\n  \n\n  \n    Ple»ôa, M.I., Gheorghe, M., Ipate, F. and Zhang, G. (2022) A key agreement protocol based on spiking neural P systems with anti-spikes. Journal of Membrane Computing, 4(4), pp. 341‚Äì351.\n  \n\n  \n    Ion, B. and Ple»ôa, M.I. (2021) Anonymous Spam Detection Service Based on Somewhat Homomorphic Encryption. International Journal of Innovative Science and Research Technology, 6(4).\n  \n\n  \n    Ple»ôa, M.I. and Ple»ôa, M.C. (2021) Self-repairing mechanical components using artificial intelligence. Scientific Bulletin ‚ÄòMircea cel Batran‚Äô Naval Academy, 24(1), pp. 17‚Äì28.\n  \n\n  \n    Ple»ôa, M.I. and Ple»ôca, C. (2020) Privacy-Preserving Clustering: A New Approach Based on Invariant Order Encryption. Journal of Military Technology, 3(2).\n  \n\n  \n    Ple»ôa, M.I. (2020) A quantum safe analysis of Helios voting system.\n  \n\n  \n    Maria-Carmen, P. and Mihail-Iulian, P. (2019) Using quantum communications for maritime signal flags. Scientific Bulletin ‚ÄòMircea cel Batran‚Äô Naval Academy, 22(1).\n  \n\n  \n    Ple»ôa, M.I. and Mihai, T. (2018) A New Quantum Encryption Scheme. Journal of Graduate Research, 4(1), p. 44.\n  \n\n  \n    Ple»ôa, M.I. (2017) Hybrid scheme for secure communications using quantum and classical mechanisms. In: 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI 2017).\n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gaussian.html",
    "href": "posts/Gaussian.html",
    "title": "A Shred of Light on the Smoothing Parameter in Lattices",
    "section": "",
    "text": "Introduction\nIf you‚Äôve delved into lattice-based cryptography, you‚Äôve probably encountered the intriguing concept of the smoothing parameter. In this post, we‚Äôll break it down step by step, ‚Äúreverse engineering‚Äù this essential lattice parameter. The aim? To help you develop a clear and intuitive understanding of it‚Äîrooted in solid mathematical principles (and maybe a touch of fun)! üòä\nOne of the standout features of lattice-based cryptography is its robust worst-case to average-case reduction security proof. Here‚Äôs the essence: when we set up a lattice-based cryptosystem, elements like secret keys and noise are generated randomly. If we assume an attacker cannot solve the underlying mathematical problem for a random instance, the system is said to have average-case security. But lattice-based cryptography takes this a step further. It provides rigorous proofs showing that solving a random (average-case) instance of certain lattice problems is at least as hard as solving the worst-case instance of a related problem. In simpler terms, if an attacker can break a random instance, they can also crack the hardest instances. This foundational property greatly boosts our confidence in the security of lattice-based cryptography.\nLet‚Äôs begin with the big picture: many security proofs in the lattice world rely on the following structure:\n\nChoose a lattice point.\n\nRandomly generate a noise vector.\n\nAdd the noise to the lattice point and reduce the result modulo the fundamental domain.\n\nTo make this more concrete, consider an example in \\(\\mathbb{R}^2\\). Here, the chosen lattice point is represented by the green point, the noise by the red vector, the sum of the two by the blue point, and the reduction modulo the fundamental domain (the hashed area) by the ‚Äòx‚Äô point.\nFor clarity and completeness, let‚Äôs introduce some notations:\n\n\\(L\\) represents the lattice.\n\n\\(B \\in \\mathbb{R}^{n \\times n}\\) is the basis of the lattice \\(L\\).\n\n\\(\\mathcal{P}(B)\\) denotes the fundamental domain of the lattice.\n\nThis structure forms the foundation for understanding key concepts in lattice-based cryptography.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\n# Function to generate lattice points\ndef generate_lattice(basis, x_range, y_range):\n    points = []\n    for i in range(x_range[0], x_range[1]):\n        for j in range(y_range[0], y_range[1]):\n            point = i * basis[:, 0] + j * basis[:, 1]\n            points.append(point)\n    return np.array(points)\n\n# Function to reduce a point modulo the fundamental domain\ndef reduce_mod_basis(point, basis):\n    inv_basis = np.linalg.inv(basis)\n    coeffs = np.dot(inv_basis, point)\n    coeffs = coeffs - np.floor(coeffs) # Reduce coefficients mod 1\n    return np.dot(basis, coeffs)\n\n# Define the lattice basis (2D) with larger vectors\nbasis = np.array([[6, 2], [3, 5]])\n\n# Generate lattice points\nlattice_points = generate_lattice(basis, x_range=(-2, 3), y_range=(-2, 3))\n\n# Select a lattice point (green dot)\nlattice_point = 1 * basis[:, 0] + 1 * basis[:, 1] # Example: (1, 1) in lattice coordinates\n\n# Add noise (larger random vector)\nnoise = np.random.uniform(-5, 5, size=2) # Larger noise\nnoisy_point = lattice_point + noise\n\n# Reduce modulo the fundamental domain\nreduced_point = reduce_mod_basis(noisy_point, basis)\n\n# Plot the lattice\nplt.figure(figsize=(10, 10))\nplt.scatter(lattice_points[:, 0], lattice_points[:, 1], color='black', s=10, label='Lattice Points')\n\n# Plot all parallelograms in the lattice\nfor point in lattice_points:\n    parallelogram = np.array([\n        point,\n        point + basis[:, 0],\n        point + basis[:, 0] + basis[:, 1],\n        point + basis[:, 1],\n        point\n    ])\n    plt.plot(parallelogram[:, 0], parallelogram[:, 1], color='orange', linestyle='dotted', linewidth=0.8)\n\n# Highlight the fundamental domain (parallelogram at the origin) with a hashed pattern\nfundamental_parallelogram = np.array([\n    [0, 0],\n    basis[:, 0],\n    basis[:, 0] + basis[:, 1],\n    basis[:, 1],\n    [0, 0]\n])\npolygon = Polygon(fundamental_parallelogram, closed=True, facecolor='none', edgecolor='orange', hatch='//', linewidth=2)\nplt.gca().add_patch(polygon)\n\n# Plot the lattice point (green dot)\nplt.scatter(lattice_point[0], lattice_point[1], color='green', s=20, label='Chosen lattice point (step 1)')\n\n# Plot the noise vector (larger arrow)\nplt.quiver(\n    lattice_point[0], lattice_point[1], noise[0], noise[1],\n    angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='Noise Vector (step 2)'\n)\n\n# Plot the noisy point\nplt.scatter(noisy_point[0], noisy_point[1], color='blue', s=20, label='Noisy Point')\n\n# Plot the reduced point\nplt.scatter(reduced_point[0], reduced_point[1], color='purple', s=20, marker='x', label='Reduced Point (step 3)')\n\n# Mark the basis vectors\nplt.quiver(\n    0, 0, basis[0, 0], basis[1, 0],\n    angles='xy', scale_units='xy', scale=1, color='magenta', width=0.005, label='Basis Vector 1'\n)\nplt.quiver(\n    0, 0, basis[0, 1], basis[1, 1],\n    angles='xy', scale_units='xy', scale=1, color='cyan', width=0.005, label='Basis Vector 2'\n)\n\n# Add labels and legend\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.title('Lattice Visualization with Hashed Fundamental Domain')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.axis('equal')\n\n# Zoom in on a smaller region of the lattice\nplt.axis('auto')\nplt.xlim(-10, 20)\nplt.ylim(-10, 20)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe problem\nNow, let‚Äôs introduce some constraints:\n\nWe want the expected norm of the error vector to be a specific value, denoted as \\(s\\).\n\nAs with most things in cryptography, we want the result of the reduction to appear as if it were sampled from a uniform distribution.\n\nWhy do we need these properties? Well, that‚Äôs a story for another article! üòä\nAt first glance, one might think: ‚ÄúIf we want vectors that look uniformly distributed, why not just sample each coordinate from a uniform distribution?‚Äù While this approach works perfectly for the second property, it fails to satisfy the first. Sampling each coordinate uniformly results in vectors with norms that deviate significantly from the desired value \\(s\\).\nTo focus on the first property, one solution is to sample each coordinate from a Gaussian distribution with a standard deviation of \\(s / \\sqrt{n}\\), where \\(n\\) is the number of coordinates. This ensures the expected norm of the vector is \\(s\\). But what about the ‚Äúuniform-like‚Äù appearance?\nLet‚Äôs build some intuition. A lattice has a clear periodic structure. Looking at the earlier example, we see the fundamental domain (a single cell) repeating endlessly to cover the entire space. This means any point in space can be expressed as the sum of a point in the fundamental domain and a lattice point.\nFrom another perspective, if we repeatedly translate the fundamental domain by lattice points, we cover the entire space. Thus, every point can be uniquely written as the sum of a lattice point and a point within the fundamental domain. This is illustrated in the figure below and can be expressed formally as:\n\\[ w = t + v, \\text{ for a unique } t \\in \\mathcal{P}(B) \\text{ and a unique } v \\in L. \\]\nThis decomposition is key to understanding how we achieve both properties simultaneously.\n\n\nCode\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\n# Function to generate lattice points\ndef generate_lattice(basis, x_range, y_range):\n    points = []\n    for i in range(x_range[0], x_range[1]):\n        for j in range(y_range[0], y_range[1]):\n            point = i * basis[:, 0] + j * basis[:, 1]\n            points.append(point)\n    return np.array(points)\n\n# Function to reduce a point modulo the fundamental domain\ndef reduce_mod_basis(point, basis):\n    inv_basis = np.linalg.inv(basis)\n    coeffs = np.dot(inv_basis, point)\n    lattice_point = np.dot(basis, np.floor(coeffs))  # Lattice point\n    fundamental_point = point - lattice_point       # Point in the fundamental domain\n    return fundamental_point, lattice_point\n\n# Define the lattice basis (2D)\nbasis = np.array([[6, 2], [3, 5]])\n\n# Generate lattice points\nlattice_points = generate_lattice(basis, x_range=(-3, 4), y_range=(-3, 4))\n\n# Define a point closer to the origin\noutside_point = np.array([8, 7])  # Example point closer to the origin\n\n# Decompose the point into a fundamental domain point and a lattice point\nfundamental_point, lattice_point = reduce_mod_basis(outside_point, basis)\n\n# Plot the lattice\nplt.figure(figsize=(10, 10))\nplt.scatter(lattice_points[:, 0], lattice_points[:, 1], color='black', s=10, label='Lattice Points')\n\n# Highlight the fundamental domain (parallelogram at the origin)\nfundamental_parallelogram = np.array([\n    [0, 0],\n    basis[:, 0],\n    basis[:, 0] + basis[:, 1],\n    basis[:, 1],\n    [0, 0]\n])\npolygon = Polygon(fundamental_parallelogram, closed=True, facecolor='none', edgecolor='orange', hatch='//', linewidth=2)\nplt.gca().add_patch(polygon)\n\n# Plot the point outside the fundamental domain\nplt.scatter(outside_point[0], outside_point[1], color='blue', s=50, label='Point Outside Fundamental Domain')\n\n# Plot the fundamental domain point\nplt.scatter(fundamental_point[0], fundamental_point[1], color='purple', s=50, label='Point in Fundamental Domain')\n\n# Plot the lattice point\nplt.scatter(lattice_point[0], lattice_point[1], color='red', s=50, label='Lattice Point')\n\n# Draw vector from origin to the outside point\nplt.quiver(\n    0, 0,\n    outside_point[0], outside_point[1],\n    angles='xy', scale_units='xy', scale=1, color='blue', width=0.005, label='Vector: Origin to Point'\n)\n\n# Draw vector from origin to the fundamental domain point\nplt.quiver(\n    0, 0,\n    fundamental_point[0], fundamental_point[1],\n    angles='xy', scale_units='xy', scale=1, color='purple', width=0.005, label='Vector: Origin to Reduction (Fundamental Domain)'\n)\n\n# Draw vector from origin to the lattice point\nplt.quiver(\n    0, 0,\n    lattice_point[0], lattice_point[1],\n    angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='Vector: Origin to Lattice Point'\n)\n\n# Add support lines to form the parallelogram\n# Line from lattice point to the outside point\nplt.plot(\n    [lattice_point[0], outside_point[0]],\n    [lattice_point[1], outside_point[1]],\n    color='gray', linestyle='--', linewidth=1, label='Support Line'\n)\n\n# Line from fundamental point to the outside point\nplt.plot(\n    [fundamental_point[0], outside_point[0]],\n    [fundamental_point[1], outside_point[1]],\n    color='gray', linestyle='--', linewidth=1\n)\n\n# Add labels and legend\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.title('Decomposition of a Point with Parallelogram Support Lines')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.axis('equal')\n\n# Adjust zoom level for better visualization\nplt.axis('auto')\nplt.xlim(-5, 15)\nplt.ylim(-5, 15)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGaussian reduction\nThis equivalence relation naturally emerges: two points are considered equivalent if they map to the same point in the fundamental domain. In other words, two points are equivalent if they differ by a lattice vector.\nNow, let‚Äôs revisit the Gaussian distribution, denoted by \\(\\mathcal{G}_s\\), which describes the probability distribution of vectors with an expected norm \\(s\\). For each point \\(x \\in \\mathbb{R}^n\\), \\(\\mathcal{G}_s\\) assigns a probability density value \\(\\mathcal{G}_s(x)\\).\nThis leads to an intriguing question:\nHow does the Gaussian distribution behave under the equivalence relation?\nWhat is the density value assigned to a point in the fundamental domain?\nRecall that for every point \\(t \\in \\mathcal{P}(B)\\) (the fundamental domain), there are infinitely many equivalent points in \\(\\mathbb{R}^n\\) that differ by lattice vectors. For instance, \\(\\mathcal{G}_s\\) assigns a density value to \\(t\\) and another density value to each equivalent point \\(w = t + v\\), where \\(v\\) is any lattice vector. However, under the equivalence relation, all these points \\(w\\) are treated as ‚Äúthe same‚Äù as \\(t\\). So, how do we assign a single density value to \\(t\\)?\nA natural solution is to define a new probability distribution, \\(\\mathcal{D}_s\\), restricted to the fundamental domain. The density value assigned by \\(\\mathcal{D}_s\\) to a point \\(t \\in \\mathcal{P}(B)\\) is the sum of all density values assigned by \\(\\mathcal{G}_s\\) to its equivalent points \\(w = t + v\\), for all \\(v \\in L\\). Formally, this is written as:\n\\[ \\mathcal{D}_s(x) = \\sum_{z \\in L} \\mathcal{G}_s(x + z) \\]\nThis construction ensures that \\(\\mathcal{D}_s\\) respects the equivalence relation and is properly defined over the fundamental domain.\nLet‚Äôs take another look at our goal. We want to sample an error vector from a Gaussian distribution with an expected norm of \\(s\\). However, after reducing this error to the fundamental domain, it should appear as though it was sampled from a uniform distribution. In other words, while the error is initially drawn from \\(\\mathcal{G}_s\\), we want the resulting distribution \\(\\mathcal{D}_s\\) to be indistinguishable from a uniform distribution.\nTo build intuition, let‚Äôs consider a simple lattice, \\(\\mathbb{Z}\\), with the fundamental domain \\([0, 1)\\). We‚Äôll explore how \\(\\mathcal{D}_s\\) behaves for different values of \\(s\\). For small values of \\(s\\) (e.g., \\(0.1\\), \\(0.2\\)), \\(\\mathcal{D}_s\\) is far from uniform‚Äîit has noticeable peaks and valleys, unlike the flat line of a uniform distribution. But as \\(s\\) increases, something fascinating happens: \\(\\mathcal{D}_s\\) starts to flatten out, gradually resembling a uniform distribution. For sufficiently large \\(s\\), \\(\\mathcal{D}_s\\) becomes nearly indistinguishable from uniform.\nThis behavior is key to achieving both properties: maintaining the expected norm of \\(s\\) while ensuring the reduced error appears uniform.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gaussian function G_s\ndef G_s(x, s):\n    scale = np.sqrt(2 * np.pi) * s\n    return (1 / scale) * np.exp(-np.pi * (x / scale) ** 2)\n\n# Periodic sum D_s over the integer lattice\ndef D_s(x, s, num_terms=100):\n    return sum(G_s(x - z, s) for z in range(-num_terms, num_terms + 1))\n\n# Static plot for multiple values of s\ndef plot_static_Gs_Ds(s_values):\n    fig, axs = plt.subplots(len(s_values), 2, figsize=(12, 3 * len(s_values)))\n\n    for i, s in enumerate(s_values):\n        # Plot G_s over the real line\n        x1 = np.linspace(-4, 4, 1000)\n        axs[i, 0].plot(x1, G_s(x1, s), label=f'$G_s$, s={s:.2f}')\n        axs[i, 0].set_title(f\"$G_s(x)$ for s={s:.2f}\")\n        axs[i, 0].set_ylabel(\"$G_s(x)$\")\n        axs[i, 0].legend()\n        axs[i, 0].grid(True)\n\n        # Plot D_s over extended domain to show multiple periods\n        x2 = np.linspace(0, 1, 1500)\n        y2 = [D_s(val, s) for val in x2]\n        axs[i, 1].plot(x2, y2, label=f'$D_s$, s={s:.2f}')\n        axs[i, 1].set_title(f\"$D_s(x)$ for s={s:.2f}\")\n        axs[i, 1].set_xlabel(\"x\")\n        axs[i, 1].set_ylabel(\"$D_s(x)$\")\n        axs[i, 1].legend()\n        axs[i, 1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Values of s to plot\ns_values = [0.2, 0.7, 1.27, 1.5]\nplot_static_Gs_Ds(s_values)\n\n\n\n\n\n\n\n\n\nTo summarize the big picture: our goal is to sample error vectors from a Gaussian distribution to achieve an expected norm of \\(s\\), while ensuring that the error appears uniformly distributed when reduced modulo the fundamental domain. The intuitive way to meet both requirements is to make the Gaussian distribution sufficiently wide.\nThis makes sense intuitively: a Gaussian distribution is defined by its characteristic ‚Äúhump.‚Äù When the distribution is wide enough and mapped into a small cell (the fundamental domain), the ‚Äúhump‚Äù effect diminishes, producing a distribution that closely resembles uniformity.\nNow, we still have one crucial question to address: how ‚Äúwide‚Äù does the Gaussian distribution need to be for \\(\\mathcal{D}_s\\) to appear uniform? To answer this, we need a way to measure how ‚Äúuniform‚Äù \\(\\mathcal{D}_s\\) actually is. Here‚Äôs a hint: let‚Äôs visualize \\(\\mathcal{D}_s\\) by plotting it over the entire space. This will give us valuable insights into its behavior and help us determine the required ‚Äúwidth.‚Äù\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gaussian function G_s\ndef G_s(x, s):\n    scale = np.sqrt(2 * np.pi) * s\n    return (1 / scale) * np.exp(-np.pi * (x / scale) ** 2)\n\n# Periodic sum D_s over the integer lattice\ndef D_s(x, s, num_terms=100):\n    return sum(G_s(x - z, s) for z in range(-num_terms, num_terms + 1))\n\n# Static plot for multiple values of s\ndef plot_static_Gs_Ds(s_values):\n    fig, axs = plt.subplots(len(s_values), 2, figsize=(12, 3 * len(s_values)))\n\n    for i, s in enumerate(s_values):\n        # Plot G_s over the real line\n        x1 = np.linspace(-4, 4, 1000)\n        axs[i, 0].plot(x1, G_s(x1, s), label=f'$G_s$, s={s:.2f}')\n        axs[i, 0].set_title(f\"$G_s(x)$ for s={s:.2f}\")\n        axs[i, 0].set_ylabel(\"$G_s(x)$\")\n        axs[i, 0].legend()\n        axs[i, 0].grid(True)\n\n        # Plot D_s over extended domain to show multiple periods\n        x2 = np.linspace(-3, 3, 1500)\n        y2 = [D_s(val, s) for val in x2]\n        axs[i, 1].plot(x2, y2, label=f'$D_s$, s={s:.2f}')\n        axs[i, 1].set_title(f\"$D_s(x)$ for s={s:.2f}\")\n        axs[i, 1].set_xlabel(\"x\")\n        axs[i, 1].set_ylabel(\"$D_s(x)$\")\n        axs[i, 1].legend()\n        axs[i, 1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Values of s to plot\ns_values = [0.2, 0.7, 1.27, 1.5]\nplot_static_Gs_Ds(s_values)\n\n\n\n\n\n\n\n\n\n\n\nFourier\nIt looks a lot like a periodic function, doesn‚Äôt it? This observation doesn‚Äôt change what we‚Äôve noted about \\(s\\): as \\(s\\) increases, \\(\\mathcal{D}_s\\) becomes increasingly uniform.\nThis periodicity naturally leads us to think about Fourier analysis.\nConsider a ‚Äúnice‚Äù periodic function \\(f: \\mathcal{P}(B) \\rightarrow \\mathbb{R}\\). Why \\(\\mathcal{P}(B)\\) and not \\(\\mathbb{R}^n\\)? Well, since the function is periodic over the lattice, it‚Äôs sufficient to define it over a single lattice cell (the fundamental domain). Fourier transform allows us to express \\(f\\) as a sum of periodic basis functions of the form \\(x \\mapsto e^{2\\pi i \\langle x, y \\rangle}\\).\nLet‚Äôs visualize some of these basis functions‚Äîor at least the real part of them‚Äîto build intuition!\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define parameters\nx = np.linspace(0, 1, 1000)  # x values from 0 to 1\ny_values = [1, 2, 4]  # Different y values\n\n# Plot basis functions\nplt.figure(figsize=(10, 6))\nfor y in y_values:\n    basis_function = np.exp(2j * np.pi * y * x)  # Complex exponential\n    plt.plot(x, np.real(basis_function), label=f'y = {y}')  # Plot real part\n\n# Customize the plot\nplt.title('Basis Functions for Frequencies (y)')\nplt.xlabel('x')\nplt.ylabel('Real Part of Basis Function')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe‚Äôve plotted some basis functions for \\(y = 1\\), \\(y = 2\\), and \\(y = 4\\), but why not try \\(y = \\sqrt{2}\\) or \\(y = 1.5\\)? What determines the allowed values for the frequencies? The answer lies in the periodicity of the basis functions:\n\\[ e^{2\\pi i \\langle x, y \\rangle} = e^{2\\pi i \\langle x+v, y \\rangle}, \\quad \\text{for all } v \\in L. \\]\nExpanding this, we get:\n\\[ e^{2\\pi i \\langle x+v, y \\rangle} = e^{2\\pi i \\langle x, y \\rangle} e^{2\\pi i \\langle v, y \\rangle}, \\]\nwhich implies:\n\\[ e^{2\\pi i \\langle v, y \\rangle} = 1. \\]\nThis condition holds when \\(2\\pi i \\langle v, y \\rangle = 0\\), or equivalently, when \\(\\langle v, y \\rangle \\in \\mathbb{Z}\\) for all \\(v \\in L\\).\nIn other words, the allowed frequencies are vectors \\(y \\in \\mathbb{R}^n\\) such that their dot product with every point in the lattice \\(L\\) is an integer. Interestingly, these frequencies themselves form a lattice, called the dual lattice, denoted by \\(L^*\\).\nLet‚Äôs return to our function \\(f\\), which can be expressed as:\n\\[ f(x) = \\sum_{y \\in L^*} \\hat{f}(y) e^{2\\pi i \\langle x, y \\rangle}, \\]\nwhere the Fourier coefficients \\(\\hat{f}(y)\\) are given by:\n\\[ \\hat{f}(y) = \\frac{1}{\\det(L)} \\int_{\\mathcal{P}(B)} f(x) e^{-2\\pi i \\langle x, y \\rangle} \\, dx. \\]\nHere, \\(\\hat{f}(y)\\) represents the ‚Äúweight‚Äù of the basis function \\(e^{2\\pi i \\langle x, y \\rangle}\\) in the decomposition. While this may seem abstract, in our case, \\(f\\) is simply the Gaussian distribution \\(\\mathcal{D}_s(x)\\), which represents the reduced Gaussian over the fundamental domain. For this specific function, the Fourier coefficients are straightforward to compute. Ignoring the constant \\(\\det(L)\\) for simplicity, we have:\n\\[ \\hat{\\mathcal{D}}_s(y) = e^{-\\pi s^2 |y|^2}. \\]\nWhy does this matter? Our goal is to measure how ‚Äúuniform‚Äù \\(\\mathcal{D}_s\\) is. Intuitively, if \\(\\mathcal{D}_s\\) were perfectly uniform, its Fourier transform would have only one non-zero coefficient at \\(y = 0\\), with \\(\\hat{\\mathcal{D}}_s(0) = 1\\). All other coefficients would be zero, as a uniform distribution requires no additional frequencies to describe it.\nIn general, we can define a ‚Äúuniformity metric‚Äù for \\(\\mathcal{D}_s\\) by summing the Fourier coefficients for all non-zero frequencies. The smaller this sum, the closer \\(\\mathcal{D}_s\\) is to uniform. This metric is expressed as:\n\\[ \\rho_{1/s}(L^* \\setminus \\{0\\}) = \\sum_{y \\in L^* \\setminus \\{0\\}} e^{-\\pi |y|^2 s^2}. \\]\nThis notation aligns with standard conventions, where \\(\\rho\\) represents the Gaussian mass. The goal in applications is to ensure that \\(\\mathcal{D}_s\\) is sufficiently uniform by keeping this metric below a chosen threshold \\(\\epsilon\\):\n\\[ \\rho_{1/s}(L^* \\setminus \\{0\\}) &lt; \\epsilon. \\]\nWhile perfect uniformity is unattainable, this approach provides a practical and rigorous way to ensure \\(\\mathcal{D}_s\\) is ‚Äúuniform enough‚Äù for cryptographic security.\n\n\nConclusions\nLet‚Äôs circle back to our motivation: we aimed to sample an error vector with a specific norm that appears uniformly distributed when reduced modulo the fundamental domain. The solution? Sample the vector from a Gaussian distribution \\(\\mathcal{G}_s\\) and reduce it to the fundamental domain. We observed that as the standard deviation \\(s\\) increases, the reduced distribution becomes more uniform. To measure this uniformity, we introduced the metric \\(\\rho_{1/s}(L^* \\setminus \\{0\\})\\).\nNow, for the final piece: the smoothing parameter of a lattice, denoted by \\(\\eta_{\\epsilon}(L)\\), is defined as the smallest \\(s\\) such that:\n\\[ \\rho_{1/s}(L^* \\setminus \\{0\\}) &lt; \\epsilon. \\]\nIn this article, we‚Äôve developed some intuition around the smoothing parameter. But the real magic lies in how Regev used this concept in one of the most groundbreaking security reductions in lattice-based cryptography. Here‚Äôs a teaser: it connects to the shortest vector problem on the dual lattice. Curious? Stay tuned for the next chapter of this fascinating journey!\nDisclaimer: This python code was generated by GenAI.\n\n\nResources\nLattices Part II ‚Äî Dual Lattices, Fourier Transform, Smoothing Parameter, Public Key Encryption\nWorst-case to Average-case Reductions based on Gaussian Measures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Algorithms",
    "section": "",
    "text": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)\n\n\n\nfoundations\n\n\n\n\n\n\n\n\n\nJan 7, 2026\n\n\nMihail Plesa\n\n\n\n\n\n\n\n\n\n\n\n\nA Shred of Light on the Smoothing Parameter in Lattices\n\n\n\ncryptography\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nMihail Plesa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Senior Research Scientist at Orange Services, conducting research on system privacy and security."
  },
  {
    "objectID": "posts/fun.html",
    "href": "posts/fun.html",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "",
    "text": "With a bit of free time on my hands during the holiday break, I decided to embark on a mathematical adventure into the foundations of combinatorial algorithms. And what better companion for this journey than the classic text, A Walk Through Combinatorics and Graph Theory by Mikl√≥s B√≥na?\nI‚Äôve made my way through the first few chapters, and in this post, I‚Äôll share my take on them‚Äîpresented with a twist. While the book itself is excellent, it can sometimes lean toward the formal side, making the underlying intuition a bit elusive. Nevertheless, its clear structure and step-by-step introduction of concepts make it a fantastic resource for beginners.\nSo, if you‚Äôre ready to explore the world of combinatorics from a fresh perspective, join me on this walk! I hope you enjoy the journey as much as I did. üòä"
  },
  {
    "objectID": "posts/fun.html#a-holiday-dive-into-combinatorics",
    "href": "posts/fun.html#a-holiday-dive-into-combinatorics",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "",
    "text": "With a bit of free time on my hands during the holiday break, I decided to embark on a mathematical adventure into the foundations of combinatorial algorithms. And what better companion for this journey than the classic text, A Walk Through Combinatorics and Graph Theory by Mikl√≥s B√≥na?\nI‚Äôve made my way through the first few chapters, and in this post, I‚Äôll share my take on them‚Äîpresented with a twist. While the book itself is excellent, it can sometimes lean toward the formal side, making the underlying intuition a bit elusive. Nevertheless, its clear structure and step-by-step introduction of concepts make it a fantastic resource for beginners.\nSo, if you‚Äôre ready to explore the world of combinatorics from a fresh perspective, join me on this walk! I hope you enjoy the journey as much as I did. üòä"
  },
  {
    "objectID": "posts/fun.html#how-many-ways-can-mathematicians-line-up-for-coffee",
    "href": "posts/fun.html#how-many-ways-can-mathematicians-line-up-for-coffee",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "How Many Ways Can Mathematicians Line Up for Coffee?",
    "text": "How Many Ways Can Mathematicians Line Up for Coffee?\nLet us commence our mathematical adventure with a deceptively simple question: Given \\(n\\) individuals, in how many distinct ways can we arrange them in a single line? Imagine, if you will, a group of eager volunteers‚Äîperhaps mathematicians at a conference‚Äîawaiting their turn to stand in line for coffee.\nFor the coveted first spot in the line, we have \\(n\\) enthusiastic candidates. Once that position is filled, only \\(n-1\\) hopefuls remain for the second spot, as one lucky individual is already sipping their metaphorical coffee at the front. This process continues, with each subsequent spot having one fewer option, until the final position, which is reserved for the last remaining person (who, let‚Äôs be honest, probably didn‚Äôt want to stand in line anyway).\nMultiplying these choices together, we obtain the grand total of possible arrangements:\n\\[ n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 \\]\nThis mathematical mouthful is succinctly denoted as \\(n!\\) (read as ‚Äún factorial‚Äù). In other words, \\(n!\\) is the number of ways to organize \\(n\\) unique individuals into a line‚Äîno coffee required, but a dash of combinatorial curiosity is highly recommended!\nBy convention, we define \\(0! = 1\\). At first glance, this might seem as peculiar as decaf coffee at a mathematicians‚Äô conference, but let‚Äôs explore why this makes perfect sense.\nImagine we have two coffee machines. In front of the first machine, there are \\(n\\) mathematicians eagerly awaiting their caffeine fix, while \\(m\\) mathematicians queue up at the second machine. The question arises: in how many ways can we form two separate lines, one for each machine? The answer is straightforward‚Äîthere are \\(n!\\) possible arrangements for the first line and \\(m!\\) for the second, so the total number of ways to organize both lines is:\n\\[ n! \\times m! \\]\nNow, let‚Äôs introduce a plot twist. Suppose all \\(m\\) mathematicians at the second coffee machine suddenly decide to abandon their quest for coffee and leave the line. The second line is now empty, but our original question remains: in how many ways can we form two lines? For the first machine, we still have \\(n!\\) possible arrangements. For the second, with zero mathematicians, how many ways can we arrange an empty line? By our earlier formula, the answer must be:\n\\[ n! \\times 0! \\]\nBut logically, there is exactly one way to arrange nothing at all‚Äîby doing nothing! Thus, for our formula to remain consistent, we must have \\(0! = 1\\). It‚Äôs a mathematical nod to the power of nothingness: even an empty line counts as one arrangement!"
  },
  {
    "objectID": "posts/fun.html#from-lineups-to-teams-counting-unique-combinations",
    "href": "posts/fun.html#from-lineups-to-teams-counting-unique-combinations",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "From Lineups to Teams: Counting Unique Combinations",
    "text": "From Lineups to Teams: Counting Unique Combinations\nNow, let‚Äôs up the ante: suppose we wish to divide our \\(n\\) mathematicians into \\(t\\) disjoint teams. The first team will have \\(a_1\\) members, the second team \\(a_2\\) members, and so on, until the \\(t\\)-th team, which has \\(a_t\\) members. Since no mathematician is allowed to clone themselves (yet), each person can only be in one team, so we must have:\n\\[ a_1 + a_2 + \\cdots + a_t = n \\]\nA natural question arises: in how many ways can we organize our \\(n\\) mathematicians into these \\(t\\) teams?\nOne intuitive approach is to line up all the mathematicians in some order and then assign the first \\(a_1\\) people to the first team, the next \\(a_2\\) to the second team, and so on. Since there are \\(n!\\) ways to arrange \\(n\\) people in a line, it might be tempting to declare that there are \\(n!\\) ways to form the teams.\nHowever, there‚Äôs a subtle catch: within each team, the order of the members doesn‚Äôt matter. The team consisting of Alice, Bob, and Eve is indistinguishable from the team of Bob, Eve, and Alice‚Äîunless, of course, you‚Äôre keeping score of who gets their coffee first.\nThis means that our \\(n!\\) possible lineups will generate many duplicate team configurations. Why? Because within each team, the order of the members doesn‚Äôt matter. For example, if the first team has \\(a_1\\) members, any of the \\(a_1!\\) ways to arrange those people among themselves will result in the exact same team. The same logic applies to the second team: there are \\(a_2!\\) ways to rearrange its members without changing the team itself. This continues for each team, all the way up to the \\(t\\)-th team, which can be internally rearranged in \\(a_t!\\) ways.\nTo find the total number of duplicate arrangements, we multiply these possibilities together:\n\\[ a_1! \\times a_2! \\times \\cdots \\times a_t! \\]\nThis product represents all the ways we can shuffle the members within each team, across all teams, without actually creating a new team configuration. So, to count only the truly distinct ways to form the teams, we divide the total number of lineups, \\(n!\\), by this product:\n\\[ \\frac{n!}{a_1! \\times a_2! \\times \\cdots \\times a_t!} \\]\nIn other words, we‚Äôre correcting for all the ‚Äúinternal shuffling‚Äù that doesn‚Äôt change the teams themselves."
  },
  {
    "objectID": "posts/fun.html#choosing-subsets-when-order-doesnt-matter",
    "href": "posts/fun.html#choosing-subsets-when-order-doesnt-matter",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "Choosing Subsets: When Order Doesn‚Äôt Matter",
    "text": "Choosing Subsets: When Order Doesn‚Äôt Matter\nNow, let‚Äôs zoom in on a particularly famous case of our team-forming formula: the binomial coefficient. Some of you may already feel a sense of d√©j√† vu!\nSuppose we set the number of teams to \\(t = 2\\), and let the first team have \\(k\\) members (that is, \\(a_1 = k\\)). By necessity, the second team will have \\(n - k\\) members, since \\(a_2 = n - k\\). Plugging these values into our earlier formula, we get:\n\\[ \\frac{n!}{k! (n-k)!} \\]\nThis expression is known as the binomial coefficient, and is commonly denoted as \\(\\binom{n}{k}\\). It represents the number of ways to choose a subset of \\(k\\) individuals from a set of \\(n\\)‚Äîor, in less formal terms, the number of ways to assemble a team of \\(k\\) mathematicians from a larger group of \\(n\\).\nBut why do we call this a ‚Äúset‚Äù? Because, in a set, the order of the elements doesn‚Äôt matter‚Äîjust as in our team-forming scenario above. Whether Alice, Bob, and Eve are chosen in that order or in any other, it‚Äôs the same team. So, \\(\\binom{n}{k}\\) counts the number of unordered groups of \\(k\\) people you can select from \\(n\\) candidates‚Äîa mathematical staple as classic as coffee at a conference!"
  },
  {
    "objectID": "posts/fun.html#the-binomial-theorem-vowels-and-the-magic-of-counting-twice",
    "href": "posts/fun.html#the-binomial-theorem-vowels-and-the-magic-of-counting-twice",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "The Binomial Theorem, Vowels, and the Magic of Counting Twice",
    "text": "The Binomial Theorem, Vowels, and the Magic of Counting Twice\nNow, for my favorite part of this post: story proofs! Throughout our mathematical education‚Äîwhether in high school, undergrad, or even grad school‚Äîwe‚Äôre often presented with proofs that are formal, symbol-heavy, and, let‚Äôs be honest, sometimes a bit intimidating. But here‚Äôs the secret: you don‚Äôt always need a blizzard of symbols to make a proof rigorous. Sometimes, a good story is all you need.\nThis idea is beautifully championed by Prof.¬†Joe Blitzstein in his legendary Stat 110 course at Harvard. Instead of drowning in algebraic notation, let‚Äôs prove a classic result with a story. Consider the famous Binomial Theorem:\n\\[ (x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} \\]\nDon‚Äôt ask me for a ‚Äústandard‚Äù proof‚ÄîI‚Äôm not sure I know one! But I can offer a proof that‚Äôs intuitive, makes perfect sense, and is as solid as any formal argument.\nThe Story:\nSuppose you have a vocabulary of size \\(v\\). How many \\(n\\)-letter words can you form using this vocabulary? For each letter in the word, you have \\(v\\) choices, so the total number of possible words is simply \\(v^n\\).\nNow, let‚Äôs count the same thing in a more nuanced way. Suppose your vocabulary consists of \\(x\\) vowels and \\(y\\) consonants, so \\(x + y = v\\). Let‚Äôs ask: how many \\(n\\)-letter words have exactly \\(k\\) vowels, and those vowels are placed at specific positions \\(i_1, i_2, \\ldots, i_k\\)?\nFor each of these \\(k\\) positions, you have \\(x\\) choices (any vowel), so there are \\(x^k\\) ways to assign vowels to those spots. For the remaining \\(n-k\\) positions, each can be filled with any of the \\(y\\) consonants, giving \\(y^{n-k}\\) possibilities. So, for a fixed set of \\(k\\) positions for the vowels, there are \\(x^k y^{n-k}\\) possible words.\nBut how many ways can we choose which \\(k\\) positions will be occupied by vowels? That‚Äôs exactly \\(\\binom{n}{k}\\), the number of ways to choose \\(k\\) positions out of \\(n\\).\nTherefore, the total number of \\(n\\)-letter words with exactly \\(k\\) vowels (regardless of where they appear) is:\n\\[ \\binom{n}{k} x^k y^{n-k} \\]\nTo find the total number of \\(n\\)-letter words (with any number of vowels from \\(0\\) to \\(n\\)), we sum over all possible values of \\(k\\):\n\\[ \\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} \\]\nBut we already know, from our earlier calculation, that the total number of \\(n\\)-letter words is \\(v^n\\), where \\(v = x + y\\). So, by counting the same thing in two different ways, we arrive at:\n\\[ (x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} \\]\nAnd there you have it‚Äîa proof that‚Äôs as satisfying as finding the last piece of a mathematical puzzle, all thanks to a story about vowels, consonants, and the magic of counting!"
  },
  {
    "objectID": "posts/fun.html#counting-is-just-the-beginning",
    "href": "posts/fun.html#counting-is-just-the-beginning",
    "title": "Combinatorial Algorithms: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "Counting is Just the Beginning",
    "text": "Counting is Just the Beginning\nThis post covered the first part of the book, focusing on the art and science of counting. But don‚Äôt put away your mathematical curiosity just yet‚Äîthere‚Äôs much more to explore! Stay tuned for the next chapters, where we‚Äôll dive even deeper into the fascinating world of combinatorics and beyond. üòä"
  },
  {
    "objectID": "posts/Comb-1.html",
    "href": "posts/Comb-1.html",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "posts/Comb-1.html#a-holiday-dive-into-combinatorics",
    "href": "posts/Comb-1.html#a-holiday-dive-into-combinatorics",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "A Holiday Dive into Combinatorics",
    "text": "A Holiday Dive into Combinatorics\nWith a bit of free time on my hands during the holiday break, I decided to embark on a mathematical adventure into the foundations of combinatorial algorithms. And what better companion for this journey than the classic text, A Walk Through Combinatorics and Graph Theory by Mikl√≥s B√≥na?\nI‚Äôve made my way through the first few chapters, and in this post, I‚Äôll share my take on them‚Äîpresented with a twist. While the book itself is excellent, it can sometimes lean toward the formal side, making the underlying intuition a bit elusive. Nevertheless, its clear structure and step-by-step introduction of concepts make it a fantastic resource for beginners.\nSo, if you‚Äôre ready to explore the world of combinatorics from a fresh perspective, join me on this walk! I hope you enjoy the journey as much as I did. üòä"
  },
  {
    "objectID": "posts/Comb-1.html#how-many-ways-can-mathematicians-line-up-for-coffee",
    "href": "posts/Comb-1.html#how-many-ways-can-mathematicians-line-up-for-coffee",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "How Many Ways Can Mathematicians Line Up for Coffee?",
    "text": "How Many Ways Can Mathematicians Line Up for Coffee?\nLet us commence our mathematical adventure with a deceptively simple question: Given \\(n\\) individuals, in how many distinct ways can we arrange them in a single line? Imagine, if you will, a group of eager volunteers‚Äîperhaps mathematicians at a conference‚Äîawaiting their turn to stand in line for coffee.\nFor the coveted first spot in the line, we have \\(n\\) enthusiastic candidates. Once that position is filled, only \\(n-1\\) hopefuls remain for the second spot, as one lucky individual is already sipping their metaphorical coffee at the front. This process continues, with each subsequent spot having one fewer option, until the final position, which is reserved for the last remaining person (who, let‚Äôs be honest, probably didn‚Äôt want to stand in line anyway).\nMultiplying these choices together, we obtain the grand total of possible arrangements:\n\\[ n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 \\]\nThis mathematical mouthful is succinctly denoted as \\(n!\\) (read as ‚Äún factorial‚Äù). In other words, \\(n!\\) is the number of ways to organize \\(n\\) unique individuals into a line‚Äîno coffee required, but a dash of combinatorial curiosity is highly recommended!\nBy convention, we define \\(0! = 1\\). At first glance, this might seem as peculiar as decaf coffee at a mathematicians‚Äô conference, but let‚Äôs explore why this makes perfect sense.\nImagine we have two coffee machines. In front of the first machine, there are \\(n\\) mathematicians eagerly awaiting their caffeine fix, while \\(m\\) mathematicians queue up at the second machine. The question arises: in how many ways can we form two separate lines, one for each machine? The answer is straightforward‚Äîthere are \\(n!\\) possible arrangements for the first line and \\(m!\\) for the second, so the total number of ways to organize both lines is:\n\\[ n! \\times m! \\]\nNow, let‚Äôs introduce a plot twist. Suppose all \\(m\\) mathematicians at the second coffee machine suddenly decide to abandon their quest for coffee and leave the line. The second line is now empty, but our original question remains: in how many ways can we form two lines? For the first machine, we still have \\(n!\\) possible arrangements. For the second, with zero mathematicians, how many ways can we arrange an empty line? By our earlier formula, the answer must be:\n\\[ n! \\times 0! \\]\nBut logically, there is exactly one way to arrange nothing at all‚Äîby doing nothing! Thus, for our formula to remain consistent, we must have \\(0! = 1\\). It‚Äôs a mathematical nod to the power of nothingness: even an empty line counts as one arrangement!"
  },
  {
    "objectID": "posts/Comb-1.html#from-lineups-to-teams-counting-unique-combinations",
    "href": "posts/Comb-1.html#from-lineups-to-teams-counting-unique-combinations",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "From Lineups to Teams: Counting Unique Combinations",
    "text": "From Lineups to Teams: Counting Unique Combinations\nNow, let‚Äôs up the ante: suppose we wish to divide our \\(n\\) mathematicians into \\(t\\) disjoint teams. The first team will have \\(a_1\\) members, the second team \\(a_2\\) members, and so on, until the \\(t\\)-th team, which has \\(a_t\\) members. Since no mathematician is allowed to clone themselves (yet), each person can only be in one team, so we must have:\n\\[ a_1 + a_2 + \\cdots + a_t = n \\]\nA natural question arises: in how many ways can we organize our \\(n\\) mathematicians into these \\(t\\) teams?\nOne intuitive approach is to line up all the mathematicians in some order and then assign the first \\(a_1\\) people to the first team, the next \\(a_2\\) to the second team, and so on. Since there are \\(n!\\) ways to arrange \\(n\\) people in a line, it might be tempting to declare that there are \\(n!\\) ways to form the teams.\nHowever, there‚Äôs a subtle catch: within each team, the order of the members doesn‚Äôt matter. The team consisting of Alice, Bob, and Eve is indistinguishable from the team of Bob, Eve, and Alice‚Äîunless, of course, you‚Äôre keeping score of who gets their coffee first.\nThis means that our \\(n!\\) possible lineups will generate many duplicate team configurations. Why? Because within each team, the order of the members doesn‚Äôt matter. For example, if the first team has \\(a_1\\) members, any of the \\(a_1!\\) ways to arrange those people among themselves will result in the exact same team. The same logic applies to the second team: there are \\(a_2!\\) ways to rearrange its members without changing the team itself. This continues for each team, all the way up to the \\(t\\)-th team, which can be internally rearranged in \\(a_t!\\) ways.\nTo find the total number of duplicate arrangements, we multiply these possibilities together:\n\\[ a_1! \\times a_2! \\times \\cdots \\times a_t! \\]\nThis product represents all the ways we can shuffle the members within each team, across all teams, without actually creating a new team configuration. So, to count only the truly distinct ways to form the teams, we divide the total number of lineups, \\(n!\\), by this product:\n\\[ \\frac{n!}{a_1! \\times a_2! \\times \\cdots \\times a_t!} \\]\nIn other words, we‚Äôre correcting for all the ‚Äúinternal shuffling‚Äù that doesn‚Äôt change the teams themselves."
  },
  {
    "objectID": "posts/Comb-1.html#choosing-subsets-when-order-doesnt-matter",
    "href": "posts/Comb-1.html#choosing-subsets-when-order-doesnt-matter",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "Choosing Subsets: When Order Doesn‚Äôt Matter",
    "text": "Choosing Subsets: When Order Doesn‚Äôt Matter\nNow, let‚Äôs zoom in on a particularly famous case of our team-forming formula: the binomial coefficient. Some of you may already feel a sense of d√©j√† vu!\nSuppose we set the number of teams to \\(t = 2\\), and let the first team have \\(k\\) members (that is, \\(a_1 = k\\)). By necessity, the second team will have \\(n - k\\) members, since \\(a_2 = n - k\\). Plugging these values into our earlier formula, we get:\n\\[ \\frac{n!}{k! (n-k)!} \\]\nThis expression is known as the binomial coefficient, and is commonly denoted as \\(\\binom{n}{k}\\). It represents the number of ways to choose a subset of \\(k\\) individuals from a set of \\(n\\)‚Äîor, in less formal terms, the number of ways to assemble a team of \\(k\\) mathematicians from a larger group of \\(n\\).\nBut why do we call this a ‚Äúset‚Äù? Because, in a set, the order of the elements doesn‚Äôt matter‚Äîjust as in our team-forming scenario above. Whether Alice, Bob, and Eve are chosen in that order or in any other, it‚Äôs the same team. So, \\(\\binom{n}{k}\\) counts the number of unordered groups of \\(k\\) people you can select from \\(n\\) candidates‚Äîa mathematical staple as classic as coffee at a conference!"
  },
  {
    "objectID": "posts/Comb-1.html#the-binomial-theorem-vowels-and-the-magic-of-counting-twice",
    "href": "posts/Comb-1.html#the-binomial-theorem-vowels-and-the-magic-of-counting-twice",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "The Binomial Theorem, Vowels, and the Magic of Counting Twice",
    "text": "The Binomial Theorem, Vowels, and the Magic of Counting Twice\nNow, for my favorite part of this post: story proofs! Throughout our mathematical education‚Äîwhether in high school, undergrad, or even grad school‚Äîwe‚Äôre often presented with proofs that are formal, symbol-heavy, and, let‚Äôs be honest, sometimes a bit intimidating. But here‚Äôs the secret: you don‚Äôt always need a blizzard of symbols to make a proof rigorous. Sometimes, a good story is all you need.\nThis idea is beautifully championed by Prof.¬†Joe Blitzstein in his legendary Stat 110 course at Harvard. Instead of drowning in algebraic notation, let‚Äôs prove a classic result with a story. Consider the famous Binomial Theorem:\n\\[ (x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} \\]\nDon‚Äôt ask me for a ‚Äústandard‚Äù proof‚ÄîI‚Äôm not sure I know one! But I can offer a proof that‚Äôs intuitive, makes perfect sense, and is as solid as any formal argument.\nThe Story:\nSuppose you have a vocabulary of size \\(v\\). How many \\(n\\)-letter words can you form using this vocabulary? For each letter in the word, you have \\(v\\) choices, so the total number of possible words is simply \\(v^n\\).\nNow, let‚Äôs count the same thing in a more nuanced way. Suppose your vocabulary consists of \\(x\\) vowels and \\(y\\) consonants, so \\(x + y = v\\). Let‚Äôs ask: how many \\(n\\)-letter words have exactly \\(k\\) vowels, and those vowels are placed at specific positions \\(i_1, i_2, \\ldots, i_k\\)?\nFor each of these \\(k\\) positions, you have \\(x\\) choices (any vowel), so there are \\(x^k\\) ways to assign vowels to those spots. For the remaining \\(n-k\\) positions, each can be filled with any of the \\(y\\) consonants, giving \\(y^{n-k}\\) possibilities. So, for a fixed set of \\(k\\) positions for the vowels, there are \\(x^k y^{n-k}\\) possible words.\nBut how many ways can we choose which \\(k\\) positions will be occupied by vowels? That‚Äôs exactly \\(\\binom{n}{k}\\), the number of ways to choose \\(k\\) positions out of \\(n\\).\nTherefore, the total number of \\(n\\)-letter words with exactly \\(k\\) vowels (regardless of where they appear) is:\n\\[ \\binom{n}{k} x^k y^{n-k} \\]\nTo find the total number of \\(n\\)-letter words (with any number of vowels from \\(0\\) to \\(n\\)), we sum over all possible values of \\(k\\):\n\\[ \\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} \\]\nBut we already know, from our earlier calculation, that the total number of \\(n\\)-letter words is \\(v^n\\), where \\(v = x + y\\). So, by counting the same thing in two different ways, we arrive at:\n\\[ (x + y)^n = \\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} \\]\nAnd there you have it‚Äîa proof that‚Äôs as satisfying as finding the last piece of a mathematical puzzle, all thanks to a story about vowels, consonants, and the magic of counting!"
  },
  {
    "objectID": "posts/Comb-1.html#counting-is-just-the-beginning",
    "href": "posts/Comb-1.html#counting-is-just-the-beginning",
    "title": "Algorithmic Foundations: Deep Dive into Combinatorial Analysis (Part 1)",
    "section": "Counting is Just the Beginning",
    "text": "Counting is Just the Beginning\nThis post covered the first part of the book, focusing on the art and science of counting. But don‚Äôt put away your mathematical curiosity just yet‚Äîthere‚Äôs much more to explore! Stay tuned for the next chapters, where we‚Äôll dive even deeper into the fascinating world of combinatorics and beyond. üòä"
  }
]