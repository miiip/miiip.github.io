[
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "How to implement a quantum key distribution protocol on IBM's quantum computer (a practical example of quantum programming)\n    DefCamp 2019 presentation\n  \n\n  \n    Augment cybersecurity through A.I.\n    DefCamp 2022 presentation\n  \n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Journal articles   \n\n\n  \n \n\n  \n    \n        A New Quantum Encryption Scheme\n    \n  \n \n\n  \n \n\n  \n    \n        A quantum safe analysis of Helios voting system\n    \n  \n \n\n  \n    \n        Privacy-Preserving Clustering: A New Approach Based on Invariant Order Encryption\n    \n  \n \n\n  \n    \n        Self-repairing mechanical components using artificial Intelligence\n    \n  \n \n\n  \n    \n        Anonymous Spam Detection Service Based on Somewhat Homomorphic Encryption\n    \n  \n \n\n  \n    \n        A key agreement protocol based on spiking neural P systems with anti-spikes\n    \n  \n \n\n\n Conference articles   \n\n    \n      \n        \n            Hybrid scheme for secure communications using quantum and classical mechanisms\n        \n      \n     \n    \n      \n     \n    \n      \n        \n            Using quantum communications for maritime signal flags\n        \n      \n     \n    \n      \n     \n    \n      \n     \n    \n      \n     \n    \n      \n     \n    \n      \n     \n    \n\nNo matching items"
  },
  {
    "objectID": "posts/Gaussian.html",
    "href": "posts/Gaussian.html",
    "title": "A Shred of Light on the Smoothing Parameter in Lattices",
    "section": "",
    "text": "Introduction\nIf you‚Äôve delved into lattice-based cryptography, you‚Äôve probably encountered the intriguing concept of the smoothing parameter. In this post, we‚Äôll break it down step by step, ‚Äúreverse engineering‚Äù this essential lattice parameter. The aim? To help you develop a clear and intuitive understanding of it‚Äîrooted in solid mathematical principles (and maybe a touch of fun)! üòä\nOne of the standout features of lattice-based cryptography is its robust worst-case to average-case reduction security proof. Here‚Äôs the essence: when we set up a lattice-based cryptosystem, elements like secret keys and noise are generated randomly. If we assume an attacker cannot solve the underlying mathematical problem for a random instance, the system is said to have average-case security. But lattice-based cryptography takes this a step further. It provides rigorous proofs showing that solving a random (average-case) instance of certain lattice problems is at least as hard as solving the worst-case instance of a related problem. In simpler terms, if an attacker can break a random instance, they can also crack the hardest instances. This foundational property greatly boosts our confidence in the security of lattice-based cryptography.\nLet‚Äôs begin with the big picture: many security proofs in the lattice world rely on the following structure:\n\nChoose a lattice point.\n\nRandomly generate a noise vector.\n\nAdd the noise to the lattice point and reduce the result modulo the fundamental domain.\n\nTo make this more concrete, consider an example in \\(\\mathbb{R}^2\\). Here, the chosen lattice point is represented by the green point, the noise by the red vector, the sum of the two by the blue point, and the reduction modulo the fundamental domain (the hashed area) by the ‚Äòx‚Äô point.\nFor clarity and completeness, let‚Äôs introduce some notations:\n\n\\(L\\) represents the lattice.\n\n\\(B \\in \\mathbb{R}^{n \\times n}\\) is the basis of the lattice \\(L\\).\n\n\\(\\mathcal{P}(B)\\) denotes the fundamental domain of the lattice.\n\nThis structure forms the foundation for understanding key concepts in lattice-based cryptography.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\n# Function to generate lattice points\ndef generate_lattice(basis, x_range, y_range):\n    points = []\n    for i in range(x_range[0], x_range[1]):\n        for j in range(y_range[0], y_range[1]):\n            point = i * basis[:, 0] + j * basis[:, 1]\n            points.append(point)\n    return np.array(points)\n\n# Function to reduce a point modulo the fundamental domain\ndef reduce_mod_basis(point, basis):\n    inv_basis = np.linalg.inv(basis)\n    coeffs = np.dot(inv_basis, point)\n    coeffs = coeffs - np.floor(coeffs) # Reduce coefficients mod 1\n    return np.dot(basis, coeffs)\n\n# Define the lattice basis (2D) with larger vectors\nbasis = np.array([[6, 2], [3, 5]])\n\n# Generate lattice points\nlattice_points = generate_lattice(basis, x_range=(-2, 3), y_range=(-2, 3))\n\n# Select a lattice point (green dot)\nlattice_point = 1 * basis[:, 0] + 1 * basis[:, 1] # Example: (1, 1) in lattice coordinates\n\n# Add noise (larger random vector)\nnoise = np.random.uniform(-5, 5, size=2) # Larger noise\nnoisy_point = lattice_point + noise\n\n# Reduce modulo the fundamental domain\nreduced_point = reduce_mod_basis(noisy_point, basis)\n\n# Plot the lattice\nplt.figure(figsize=(10, 10))\nplt.scatter(lattice_points[:, 0], lattice_points[:, 1], color='black', s=10, label='Lattice Points')\n\n# Plot all parallelograms in the lattice\nfor point in lattice_points:\n    parallelogram = np.array([\n        point,\n        point + basis[:, 0],\n        point + basis[:, 0] + basis[:, 1],\n        point + basis[:, 1],\n        point\n    ])\n    plt.plot(parallelogram[:, 0], parallelogram[:, 1], color='orange', linestyle='dotted', linewidth=0.8)\n\n# Highlight the fundamental domain (parallelogram at the origin) with a hashed pattern\nfundamental_parallelogram = np.array([\n    [0, 0],\n    basis[:, 0],\n    basis[:, 0] + basis[:, 1],\n    basis[:, 1],\n    [0, 0]\n])\npolygon = Polygon(fundamental_parallelogram, closed=True, facecolor='none', edgecolor='orange', hatch='//', linewidth=2)\nplt.gca().add_patch(polygon)\n\n# Plot the lattice point (green dot)\nplt.scatter(lattice_point[0], lattice_point[1], color='green', s=20, label='Chosen lattice point (step 1)')\n\n# Plot the noise vector (larger arrow)\nplt.quiver(\n    lattice_point[0], lattice_point[1], noise[0], noise[1],\n    angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='Noise Vector (step 2)'\n)\n\n# Plot the noisy point\nplt.scatter(noisy_point[0], noisy_point[1], color='blue', s=20, label='Noisy Point')\n\n# Plot the reduced point\nplt.scatter(reduced_point[0], reduced_point[1], color='purple', s=20, marker='x', label='Reduced Point (step 3)')\n\n# Mark the basis vectors\nplt.quiver(\n    0, 0, basis[0, 0], basis[1, 0],\n    angles='xy', scale_units='xy', scale=1, color='magenta', width=0.005, label='Basis Vector 1'\n)\nplt.quiver(\n    0, 0, basis[0, 1], basis[1, 1],\n    angles='xy', scale_units='xy', scale=1, color='cyan', width=0.005, label='Basis Vector 2'\n)\n\n# Add labels and legend\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.title('Lattice Visualization with Hashed Fundamental Domain')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.axis('equal')\n\n# Zoom in on a smaller region of the lattice\nplt.axis('auto')\nplt.xlim(-10, 20)\nplt.ylim(-10, 20)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe problem\nNow, let‚Äôs introduce some constraints:\n\nWe want the expected norm of the error vector to be a specific value, denoted as \\(s\\).\n\nAs with most things in cryptography, we want the result of the reduction to appear as if it were sampled from a uniform distribution.\n\nWhy do we need these properties? Well, that‚Äôs a story for another article! üòä\nAt first glance, one might think: ‚ÄúIf we want vectors that look uniformly distributed, why not just sample each coordinate from a uniform distribution?‚Äù While this approach works perfectly for the second property, it fails to satisfy the first. Sampling each coordinate uniformly results in vectors with norms that deviate significantly from the desired value \\(s\\).\nTo focus on the first property, one solution is to sample each coordinate from a Gaussian distribution with a standard deviation of \\(s / \\sqrt{n}\\), where \\(n\\) is the number of coordinates. This ensures the expected norm of the vector is \\(s\\). But what about the ‚Äúuniform-like‚Äù appearance?\nLet‚Äôs build some intuition. A lattice has a clear periodic structure. Looking at the earlier example, we see the fundamental domain (a single cell) repeating endlessly to cover the entire space. This means any point in space can be expressed as the sum of a point in the fundamental domain and a lattice point.\nFrom another perspective, if we repeatedly translate the fundamental domain by lattice points, we cover the entire space. Thus, every point can be uniquely written as the sum of a lattice point and a point within the fundamental domain. This is illustrated in the figure below and can be expressed formally as:\n\\[ w = t + v, \\text{ for a unique } t \\in \\mathcal{P}(B) \\text{ and a unique } v \\in L. \\]\nThis decomposition is key to understanding how we achieve both properties simultaneously.\n\n\nCode\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\n# Function to generate lattice points\ndef generate_lattice(basis, x_range, y_range):\n    points = []\n    for i in range(x_range[0], x_range[1]):\n        for j in range(y_range[0], y_range[1]):\n            point = i * basis[:, 0] + j * basis[:, 1]\n            points.append(point)\n    return np.array(points)\n\n# Function to reduce a point modulo the fundamental domain\ndef reduce_mod_basis(point, basis):\n    inv_basis = np.linalg.inv(basis)\n    coeffs = np.dot(inv_basis, point)\n    lattice_point = np.dot(basis, np.floor(coeffs))  # Lattice point\n    fundamental_point = point - lattice_point       # Point in the fundamental domain\n    return fundamental_point, lattice_point\n\n# Define the lattice basis (2D)\nbasis = np.array([[6, 2], [3, 5]])\n\n# Generate lattice points\nlattice_points = generate_lattice(basis, x_range=(-3, 4), y_range=(-3, 4))\n\n# Define a point closer to the origin\noutside_point = np.array([8, 7])  # Example point closer to the origin\n\n# Decompose the point into a fundamental domain point and a lattice point\nfundamental_point, lattice_point = reduce_mod_basis(outside_point, basis)\n\n# Plot the lattice\nplt.figure(figsize=(10, 10))\nplt.scatter(lattice_points[:, 0], lattice_points[:, 1], color='black', s=10, label='Lattice Points')\n\n# Highlight the fundamental domain (parallelogram at the origin)\nfundamental_parallelogram = np.array([\n    [0, 0],\n    basis[:, 0],\n    basis[:, 0] + basis[:, 1],\n    basis[:, 1],\n    [0, 0]\n])\npolygon = Polygon(fundamental_parallelogram, closed=True, facecolor='none', edgecolor='orange', hatch='//', linewidth=2)\nplt.gca().add_patch(polygon)\n\n# Plot the point outside the fundamental domain\nplt.scatter(outside_point[0], outside_point[1], color='blue', s=50, label='Point Outside Fundamental Domain')\n\n# Plot the fundamental domain point\nplt.scatter(fundamental_point[0], fundamental_point[1], color='purple', s=50, label='Point in Fundamental Domain')\n\n# Plot the lattice point\nplt.scatter(lattice_point[0], lattice_point[1], color='red', s=50, label='Lattice Point')\n\n# Draw vector from origin to the outside point\nplt.quiver(\n    0, 0,\n    outside_point[0], outside_point[1],\n    angles='xy', scale_units='xy', scale=1, color='blue', width=0.005, label='Vector: Origin to Point'\n)\n\n# Draw vector from origin to the fundamental domain point\nplt.quiver(\n    0, 0,\n    fundamental_point[0], fundamental_point[1],\n    angles='xy', scale_units='xy', scale=1, color='purple', width=0.005, label='Vector: Origin to Reduction (Fundamental Domain)'\n)\n\n# Draw vector from origin to the lattice point\nplt.quiver(\n    0, 0,\n    lattice_point[0], lattice_point[1],\n    angles='xy', scale_units='xy', scale=1, color='red', width=0.005, label='Vector: Origin to Lattice Point'\n)\n\n# Add support lines to form the parallelogram\n# Line from lattice point to the outside point\nplt.plot(\n    [lattice_point[0], outside_point[0]],\n    [lattice_point[1], outside_point[1]],\n    color='gray', linestyle='--', linewidth=1, label='Support Line'\n)\n\n# Line from fundamental point to the outside point\nplt.plot(\n    [fundamental_point[0], outside_point[0]],\n    [fundamental_point[1], outside_point[1]],\n    color='gray', linestyle='--', linewidth=1\n)\n\n# Add labels and legend\nplt.axhline(0, color='gray', linewidth=0.5)\nplt.axvline(0, color='gray', linewidth=0.5)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.5)\nplt.legend()\nplt.title('Decomposition of a Point with Parallelogram Support Lines')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.axis('equal')\n\n# Adjust zoom level for better visualization\nplt.axis('auto')\nplt.xlim(-5, 15)\nplt.ylim(-5, 15)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nGaussian reduction\nThis equivalence relation naturally emerges: two points are considered equivalent if they map to the same point in the fundamental domain. In other words, two points are equivalent if they differ by a lattice vector.\nNow, let‚Äôs revisit the Gaussian distribution, denoted by \\(\\mathcal{G}_s\\), which describes the probability distribution of vectors with an expected norm \\(s\\). For each point \\(x \\in \\mathbb{R}^n\\), \\(\\mathcal{G}_s\\) assigns a probability density value \\(\\mathcal{G}_s(x)\\).\nThis leads to an intriguing question:\nHow does the Gaussian distribution behave under the equivalence relation?\nWhat is the density value assigned to a point in the fundamental domain?\nRecall that for every point \\(t \\in \\mathcal{P}(B)\\) (the fundamental domain), there are infinitely many equivalent points in \\(\\mathbb{R}^n\\) that differ by lattice vectors. For instance, \\(\\mathcal{G}_s\\) assigns a density value to \\(t\\) and another density value to each equivalent point \\(w = t + v\\), where \\(v\\) is any lattice vector. However, under the equivalence relation, all these points \\(w\\) are treated as ‚Äúthe same‚Äù as \\(t\\). So, how do we assign a single density value to \\(t\\)?\nA natural solution is to define a new probability distribution, \\(\\mathcal{D}_s\\), restricted to the fundamental domain. The density value assigned by \\(\\mathcal{D}_s\\) to a point \\(t \\in \\mathcal{P}(B)\\) is the sum of all density values assigned by \\(\\mathcal{G}_s\\) to its equivalent points \\(w = t + v\\), for all \\(v \\in L\\). Formally, this is written as:\n\\[ \\mathcal{D}_s(x) = \\sum_{z \\in L} \\mathcal{G}_s(x + z) \\]\nThis construction ensures that \\(\\mathcal{D}_s\\) respects the equivalence relation and is properly defined over the fundamental domain.\nLet‚Äôs take another look at our goal. We want to sample an error vector from a Gaussian distribution with an expected norm of \\(s\\). However, after reducing this error to the fundamental domain, it should appear as though it was sampled from a uniform distribution. In other words, while the error is initially drawn from \\(\\mathcal{G}_s\\), we want the resulting distribution \\(\\mathcal{D}_s\\) to be indistinguishable from a uniform distribution.\nTo build intuition, let‚Äôs consider a simple lattice, \\(\\mathbb{Z}\\), with the fundamental domain \\([0, 1)\\). We‚Äôll explore how \\(\\mathcal{D}_s\\) behaves for different values of \\(s\\). For small values of \\(s\\) (e.g., \\(0.1\\), \\(0.2\\)), \\(\\mathcal{D}_s\\) is far from uniform‚Äîit has noticeable peaks and valleys, unlike the flat line of a uniform distribution. But as \\(s\\) increases, something fascinating happens: \\(\\mathcal{D}_s\\) starts to flatten out, gradually resembling a uniform distribution. For sufficiently large \\(s\\), \\(\\mathcal{D}_s\\) becomes nearly indistinguishable from uniform.\nThis behavior is key to achieving both properties: maintaining the expected norm of \\(s\\) while ensuring the reduced error appears uniform.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gaussian function G_s\ndef G_s(x, s):\n    scale = np.sqrt(2 * np.pi) * s\n    return (1 / scale) * np.exp(-np.pi * (x / scale) ** 2)\n\n# Periodic sum D_s over the integer lattice\ndef D_s(x, s, num_terms=100):\n    return sum(G_s(x - z, s) for z in range(-num_terms, num_terms + 1))\n\n# Static plot for multiple values of s\ndef plot_static_Gs_Ds(s_values):\n    fig, axs = plt.subplots(len(s_values), 2, figsize=(12, 3 * len(s_values)))\n\n    for i, s in enumerate(s_values):\n        # Plot G_s over the real line\n        x1 = np.linspace(-4, 4, 1000)\n        axs[i, 0].plot(x1, G_s(x1, s), label=f'$G_s$, s={s:.2f}')\n        axs[i, 0].set_title(f\"$G_s(x)$ for s={s:.2f}\")\n        axs[i, 0].set_ylabel(\"$G_s(x)$\")\n        axs[i, 0].legend()\n        axs[i, 0].grid(True)\n\n        # Plot D_s over extended domain to show multiple periods\n        x2 = np.linspace(0, 1, 1500)\n        y2 = [D_s(val, s) for val in x2]\n        axs[i, 1].plot(x2, y2, label=f'$D_s$, s={s:.2f}')\n        axs[i, 1].set_title(f\"$D_s(x)$ for s={s:.2f}\")\n        axs[i, 1].set_xlabel(\"x\")\n        axs[i, 1].set_ylabel(\"$D_s(x)$\")\n        axs[i, 1].legend()\n        axs[i, 1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Values of s to plot\ns_values = [0.2, 0.7, 1.27, 1.5]\nplot_static_Gs_Ds(s_values)\n\n\n\n\n\n\n\n\n\nTo summarize the big picture: our goal is to sample error vectors from a Gaussian distribution to achieve an expected norm of \\(s\\), while ensuring that the error appears uniformly distributed when reduced modulo the fundamental domain. The intuitive way to meet both requirements is to make the Gaussian distribution sufficiently wide.\nThis makes sense intuitively: a Gaussian distribution is defined by its characteristic ‚Äúhump.‚Äù When the distribution is wide enough and mapped into a small cell (the fundamental domain), the ‚Äúhump‚Äù effect diminishes, producing a distribution that closely resembles uniformity.\nNow, we still have one crucial question to address: how ‚Äúwide‚Äù does the Gaussian distribution need to be for \\(\\mathcal{D}_s\\) to appear uniform? To answer this, we need a way to measure how ‚Äúuniform‚Äù \\(\\mathcal{D}_s\\) actually is. Here‚Äôs a hint: let‚Äôs visualize \\(\\mathcal{D}_s\\) by plotting it over the entire space. This will give us valuable insights into its behavior and help us determine the required ‚Äúwidth.‚Äù\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gaussian function G_s\ndef G_s(x, s):\n    scale = np.sqrt(2 * np.pi) * s\n    return (1 / scale) * np.exp(-np.pi * (x / scale) ** 2)\n\n# Periodic sum D_s over the integer lattice\ndef D_s(x, s, num_terms=100):\n    return sum(G_s(x - z, s) for z in range(-num_terms, num_terms + 1))\n\n# Static plot for multiple values of s\ndef plot_static_Gs_Ds(s_values):\n    fig, axs = plt.subplots(len(s_values), 2, figsize=(12, 3 * len(s_values)))\n\n    for i, s in enumerate(s_values):\n        # Plot G_s over the real line\n        x1 = np.linspace(-4, 4, 1000)\n        axs[i, 0].plot(x1, G_s(x1, s), label=f'$G_s$, s={s:.2f}')\n        axs[i, 0].set_title(f\"$G_s(x)$ for s={s:.2f}\")\n        axs[i, 0].set_ylabel(\"$G_s(x)$\")\n        axs[i, 0].legend()\n        axs[i, 0].grid(True)\n\n        # Plot D_s over extended domain to show multiple periods\n        x2 = np.linspace(-3, 3, 1500)\n        y2 = [D_s(val, s) for val in x2]\n        axs[i, 1].plot(x2, y2, label=f'$D_s$, s={s:.2f}')\n        axs[i, 1].set_title(f\"$D_s(x)$ for s={s:.2f}\")\n        axs[i, 1].set_xlabel(\"x\")\n        axs[i, 1].set_ylabel(\"$D_s(x)$\")\n        axs[i, 1].legend()\n        axs[i, 1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Values of s to plot\ns_values = [0.2, 0.7, 1.27, 1.5]\nplot_static_Gs_Ds(s_values)\n\n\n\n\n\n\n\n\n\n\n\nFourier\nIt looks a lot like a periodic function, doesn‚Äôt it? This observation doesn‚Äôt change what we‚Äôve noted about \\(s\\): as \\(s\\) increases, \\(\\mathcal{D}_s\\) becomes increasingly uniform.\nThis periodicity naturally leads us to think about Fourier analysis.\nConsider a ‚Äúnice‚Äù periodic function \\(f: \\mathcal{P}(B) \\rightarrow \\mathbb{R}\\). Why \\(\\mathcal{P}(B)\\) and not \\(\\mathbb{R}^n\\)? Well, since the function is periodic over the lattice, it‚Äôs sufficient to define it over a single lattice cell (the fundamental domain). Fourier transform allows us to express \\(f\\) as a sum of periodic basis functions of the form \\(x \\mapsto e^{2\\pi i \\langle x, y \\rangle}\\).\nLet‚Äôs visualize some of these basis functions‚Äîor at least the real part of them‚Äîto build intuition!\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define parameters\nx = np.linspace(0, 1, 1000)  # x values from 0 to 1\ny_values = [1, 2, 4]  # Different y values\n\n# Plot basis functions\nplt.figure(figsize=(10, 6))\nfor y in y_values:\n    basis_function = np.exp(2j * np.pi * y * x)  # Complex exponential\n    plt.plot(x, np.real(basis_function), label=f'y = {y}')  # Plot real part\n\n# Customize the plot\nplt.title('Basis Functions for Frequencies (y)')\nplt.xlabel('x')\nplt.ylabel('Real Part of Basis Function')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe‚Äôve plotted some basis functions for \\(y = 1\\), \\(y = 2\\), and \\(y = 4\\), but why not try \\(y = \\sqrt{2}\\) or \\(y = 1.5\\)? What determines the allowed values for the frequencies? The answer lies in the periodicity of the basis functions:\n\\[ e^{2\\pi i \\langle x, y \\rangle} = e^{2\\pi i \\langle x+v, y \\rangle}, \\quad \\text{for all } v \\in L. \\]\nExpanding this, we get:\n\\[ e^{2\\pi i \\langle x+v, y \\rangle} = e^{2\\pi i \\langle x, y \\rangle} e^{2\\pi i \\langle v, y \\rangle}, \\]\nwhich implies:\n\\[ e^{2\\pi i \\langle v, y \\rangle} = 1. \\]\nThis condition holds when \\(2\\pi i \\langle v, y \\rangle = 0\\), or equivalently, when \\(\\langle v, y \\rangle \\in \\mathbb{Z}\\) for all \\(v \\in L\\).\nIn other words, the allowed frequencies are vectors \\(y \\in \\mathbb{R}^n\\) such that their dot product with every point in the lattice \\(L\\) is an integer. Interestingly, these frequencies themselves form a lattice, called the dual lattice, denoted by \\(L^*\\).\nLet‚Äôs return to our function \\(f\\), which can be expressed as:\n\\[ f(x) = \\sum_{y \\in L^*} \\hat{f}(y) e^{2\\pi i \\langle x, y \\rangle}, \\]\nwhere the Fourier coefficients \\(\\hat{f}(y)\\) are given by:\n\\[ \\hat{f}(y) = \\frac{1}{\\det(L)} \\int_{\\mathcal{P}(B)} f(x) e^{-2\\pi i \\langle x, y \\rangle} \\, dx. \\]\nHere, \\(\\hat{f}(y)\\) represents the ‚Äúweight‚Äù of the basis function \\(e^{2\\pi i \\langle x, y \\rangle}\\) in the decomposition. While this may seem abstract, in our case, \\(f\\) is simply the Gaussian distribution \\(\\mathcal{D}_s(x)\\), which represents the reduced Gaussian over the fundamental domain. For this specific function, the Fourier coefficients are straightforward to compute. Ignoring the constant \\(\\det(L)\\) for simplicity, we have:\n\\[ \\hat{\\mathcal{D}}_s(y) = e^{-\\pi s^2 |y|^2}. \\]\nWhy does this matter? Our goal is to measure how ‚Äúuniform‚Äù \\(\\mathcal{D}_s\\) is. Intuitively, if \\(\\mathcal{D}_s\\) were perfectly uniform, its Fourier transform would have only one non-zero coefficient at \\(y = 0\\), with \\(\\hat{\\mathcal{D}}_s(0) = 1\\). All other coefficients would be zero, as a uniform distribution requires no additional frequencies to describe it.\nIn general, we can define a ‚Äúuniformity metric‚Äù for \\(\\mathcal{D}_s\\) by summing the Fourier coefficients for all non-zero frequencies. The smaller this sum, the closer \\(\\mathcal{D}_s\\) is to uniform. This metric is expressed as:\n\\[ \\rho_{1/s}(L^* \\setminus \\{0\\}) = \\sum_{y \\in L^* \\setminus \\{0\\}} e^{-\\pi |y|^2 s^2}. \\]\nThis notation aligns with standard conventions, where \\(\\rho\\) represents the Gaussian mass. The goal in applications is to ensure that \\(\\mathcal{D}_s\\) is sufficiently uniform by keeping this metric below a chosen threshold \\(\\epsilon\\):\n\\[ \\rho_{1/s}(L^* \\setminus \\{0\\}) &lt; \\epsilon. \\]\nWhile perfect uniformity is unattainable, this approach provides a practical and rigorous way to ensure \\(\\mathcal{D}_s\\) is ‚Äúuniform enough‚Äù for cryptographic security.\n\n\nConclusions\nLet‚Äôs circle back to our motivation: we aimed to sample an error vector with a specific norm that appears uniformly distributed when reduced modulo the fundamental domain. The solution? Sample the vector from a Gaussian distribution \\(\\mathcal{G}_s\\) and reduce it to the fundamental domain. We observed that as the standard deviation \\(s\\) increases, the reduced distribution becomes more uniform. To measure this uniformity, we introduced the metric \\(\\rho_{1/s}(L^* \\setminus \\{0\\})\\).\nNow, for the final piece: the smoothing parameter of a lattice, denoted by \\(\\eta_{\\epsilon}(L)\\), is defined as the smallest \\(s\\) such that:\n\\[ \\rho_{1/s}(L^* \\setminus \\{0\\}) &lt; \\epsilon. \\]\nIn this article, we‚Äôve developed some intuition around the smoothing parameter. But the real magic lies in how Regev used this concept in one of the most groundbreaking security reductions in lattice-based cryptography. Here‚Äôs a teaser: it connects to the shortest vector problem on the dual lattice. Curious? Stay tuned for the next chapter of this fascinating journey!\nDisclaimer: This python code was generated by GenAI.\n\n\nResources\nLattices Part II ‚Äî Dual Lattices, Fourier Transform, Smoothing Parameter, Public Key Encryption\nWorst-case to Average-case Reductions based on Gaussian Measures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tiny Keys: A Cryptography Journey",
    "section": "",
    "text": "A Shred of Light on the Smoothing Parameter in Lattices\n\n\n\nfoundations-cryptography\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nMihail Plesa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a security researcher at Orange Services, focusing on applied cryptography."
  }
]